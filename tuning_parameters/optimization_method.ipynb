{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# batch gradient descent\n",
    "def update_parameters_with_gd(parameters, grads, learning_rate):\n",
    "    L = len(parameters)//2\n",
    "    \n",
    "    for l in range(L):\n",
    "        parameters['W'+str(l+1)] = parameters['W'+str(l+1)] - learning_rate*grads['dW'+str(l+1)]\n",
    "        parameters['b'+str(l+1)] = parameters['b'+str(l+1)] - learning_rate*grads['db'+str(l+1)]\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# batch gradient descent and stohastic gradient descent are different from each other\n",
    "# batch gradient descent trains whole training set while SGD trains only one training example at a time\n",
    "\n",
    "# BGD\n",
    "### X = data_input\n",
    "### Y = labels\n",
    "### parameters = initialize_parameters(layers_dims)\n",
    "### for i in range(0,num_iterations):\n",
    "###     a, caches = forward_propagation(X, parameters)\n",
    "###     cost = compute_cost(a, Y)\n",
    "###     grads = backward_propagation(a, caches, parameters)\n",
    "###     parameters = update_parameters(parameters, grads)\n",
    "    \n",
    "# SGD\n",
    "### X = data_input\n",
    "### Y = labels\n",
    "### parameters = initialize_parameters(layers_dims)\n",
    "### for i in range(0, num_iterations):\n",
    "###     for j in range(0, m):\n",
    "###         a, caches = forward_propagation(X[:,j], parameters)\n",
    "###         cost = compute_cost(a, Y[:,j])\n",
    "###         grads = backward_propagation(a, caches, parameters)\n",
    "###         parameters = update_parameters(parameters, grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# bulid mini-batches\n",
    "# need shuffle and partition\n",
    "def random_mini_batches(X, Y, mini_batch_size = 64):\n",
    "    m = X.shape[1]\n",
    "    mini_batches = []\n",
    "    \n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[:, permutation]\n",
    "    shuffled_Y = Y[:, permutation].reshape((1,m))\n",
    "    \n",
    "    num_complete_minibatches = math.floor(m/mini_batch_size)\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[:,k*mini_batch_size:(k+1)*mini_batch_size]\n",
    "        mini_batch_Y = shuffled_Y[:,k*mini_batch_size:(k+1)*mini_batch_size]\n",
    "        \n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[:,num_complete_minibatches*mini_batch_size:m]\n",
    "        mini_batch_Y = shuffled_Y[:,num_complete_minibatches*mini_batch_size:m]\n",
    "        \n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "        \n",
    "    return mini_batches\n",
    "# size of mini_batch id always choose powers of 2 大概是和内存有关系"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# momentum\n",
    "def initialize_volecity(parameters):\n",
    "    L = len(parameters)//2\n",
    "    v = {}\n",
    "    \n",
    "    for l in range(L):\n",
    "        v['dW'+str(l+1)] = np.zeros(parameters['W'+str(l+1)].shape)\n",
    "        v['db'+str(l+1)] = np.zeros(parameters['b'+str(l+1)].shape)\n",
    "        \n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the larger beta is, the smoother the update\n",
    "# beta usually between 0.8 to 0.999, default is always to be 0.9\n",
    "def update_parameters_with_momentum(parameters, grads, v, beta, learning_rate):\n",
    "    L = len(parameters)//2\n",
    "    \n",
    "    for l in range(L):\n",
    "        v['dW'+str(l+1)] = beta*v['dW'+str(l+1)] + (1-beta)*grads['dW'+str(l+1)]\n",
    "        v['db'+str(l+1)] = beta*v['db'+str(l+1)] + (1-beta)*grads['db'+str(l+1)]\n",
    "        \n",
    "        parameters['W'+str(l+1)] = parameters['W'+str(l+1)] - learning_rate*v['dW'+str(l+1)]\n",
    "        parameters['b'+str(l+1)] = parameters['b'+str(l+1)] - learning_rate*v['db'+str(l+1)]\n",
    "        \n",
    "    return parameters, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Adam\n",
    "def innitialize_adam(parameters):\n",
    "    L = len(parameters)//2\n",
    "    v = {}\n",
    "    s = {}\n",
    "    \n",
    "    for l in range(L):\n",
    "        v['dW'+str(l+1)] = np.zeros(parameters['W'+str(l+1)].shape)\n",
    "        v['db'+str(l+1)] = np.zeros(parameters['b'+str(l+1)].shape)\n",
    "        s['dW'+str(l+1)] = np.zeros(parameters['W'+str(l+1)].shape)\n",
    "        s['db'+str(l+1)] = np.zeros(parameters['b'+str(l+1)].shape)\n",
    "    return v, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_parameters_with_adam(parameters, grads, v, s, t, learning_rate = 0.01, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "    L = len(parameters)//2\n",
    "    v_corrected = {}\n",
    "    s_corrected = {}\n",
    "    \n",
    "    for l in range(L):\n",
    "        v['dW'+str(l+1)] = beta1*v['dW'+str(l+1)] + (1-beta1)*grads['dW'+str(l+1)]\n",
    "        v['db'+str(l+1)] = beta1*v['db'+str(l+1)] + (1-beta1)*grads['db'+str(l+1)]\n",
    "        \n",
    "        v_corrected['dW'+str(l+1)] = v['dW'+str(l+1)]/(1-beta1**t)\n",
    "        v_corrected[\"db\" + str(l+1)] = v['db'+str(l+1)]/(1-beta1**t)\n",
    "        \n",
    "        s[\"dW\" + str(l+1)] = beta2*s['dW'+str(l+1)] + (1-beta2)*(grads['dW'+str(l+1)]**2)\n",
    "        s[\"db\" + str(l+1)] = beta2*s['db'+str(l+1)] + (1-beta2)*(grads['db'+str(l+1)]**2)\n",
    "        \n",
    "        s_corrected[\"dW\" + str(l+1)] = s['dW'+str(l+1)]/(1-beta2**t)\n",
    "        s_corrected[\"db\" + str(l+1)] = s['db'+str(l+1)]/(1-beta2**t)\n",
    "        \n",
    "        parameters[\"W\" + str(l+1)] = parameters['W'+str(l+1)] - learning_rate*v_corrected['dW'+str(l+1)]/(np.sqrt(s_corrected['dW'+str(l+1)])+epsilon)\n",
    "        parameters[\"b\" + str(l+1)] = parameters['b'+str(l+1)] - learning_rate*v_corrected['db'+str(l+1)]/(np.sqrt(s_corrected['db'+str(l+1)])+epsilon)\n",
    "        \n",
    "    return parameters, v, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(X, Y, layers_dims, optimizer, learning_rate = 0.0007, mini_batch_size = 64, beta = 0.9,\n",
    "          beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8, num_epochs = 10000, print_cost = True):\n",
    "    \n",
    "    L = len(layers_dims)         \n",
    "    costs = []                      \n",
    "    t = 0                           \n",
    "    seed = 10                        \n",
    " \n",
    "    parameters = initialize_parameters(layers_dims)\n",
    "\n",
    "    if optimizer == \"gd\":\n",
    "        pass\n",
    "    elif optimizer == \"momentum\":\n",
    "        v = initialize_velocity(parameters)\n",
    "    elif optimizer == \"adam\":\n",
    "        v, s = initialize_adam(parameters)\n",
    "\n",
    "    for i in range(num_epochs):\n",
    "     \n",
    "        seed = seed + 1\n",
    "        minibatches = random_mini_batches(X, Y, mini_batch_size, seed)\n",
    "\n",
    "        for minibatch in minibatches:\n",
    "\n",
    "            (minibatch_X, minibatch_Y) = minibatch\n",
    "\n",
    "            a3, caches = forward_propagation(minibatch_X, parameters)\n",
    "\n",
    "            cost = compute_cost(a3, minibatch_Y)\n",
    "\n",
    "            grads = backward_propagation(minibatch_X, minibatch_Y, caches)\n",
    "\n",
    "            if optimizer == \"gd\":\n",
    "                parameters = update_parameters_with_gd(parameters, grads, learning_rate)\n",
    "            elif optimizer == \"momentum\":\n",
    "                parameters, v = update_parameters_with_momentum(parameters, grads, v, beta, learning_rate)\n",
    "            elif optimizer == \"adam\":\n",
    "                t = t + 1 # Adam counter\n",
    "                parameters, v, s = update_parameters_with_adam(parameters, grads, v, s,\n",
    "                                                               t, learning_rate, beta1, beta2,  epsilon)\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
